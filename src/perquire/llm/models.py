"""
Pydantic models for structured LLM outputs.

These models define the structure and validation for all LLM interactions
in Perquire, enabling type-safe, validated responses from language models.
"""

from datetime import datetime
from typing import Literal, Optional
from pydantic import BaseModel, Field, field_validator


class InvestigationQuestion(BaseModel):
    """A single investigation question with metadata."""

    question: str = Field(
        ...,
        description="The investigation question to ask",
        min_length=10,
        max_length=500
    )
    phase: Literal["exploration", "refinement", "convergence"] = Field(
        ...,
        description="The investigation phase this question belongs to"
    )
    expected_similarity_gain: float = Field(
        default=0.0,
        ge=0.0,
        le=1.0,
        description="Expected improvement in similarity score (0-1)"
    )
    rationale: str = Field(
        default="",
        description="Why this question is expected to be useful"
    )

    @field_validator('question')
    @classmethod
    def question_must_end_with_question_mark(cls, v: str) -> str:
        """Ensure questions end with a question mark."""
        if not v.strip().endswith('?'):
            return v.strip() + '?'
        return v.strip()


class QuestionBatch(BaseModel):
    """A batch of investigation questions generated by the LLM."""

    questions: list[InvestigationQuestion] = Field(
        ...,
        description="List of generated questions",
        min_length=1,
        max_length=5
    )
    strategy: str = Field(
        ...,
        description="The strategy used to generate these questions"
    )
    current_similarity: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Current similarity score that prompted these questions"
    )

    @property
    def question_texts(self) -> list[str]:
        """Get just the question strings."""
        return [q.question for q in self.questions]


class SynthesizedDescription(BaseModel):
    """A synthesized description of what the embedding represents."""

    description: str = Field(
        ...,
        description="Human-readable description of the embedding",
        min_length=20,
        max_length=1000
    )
    confidence: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Confidence level in this description (0-1)"
    )
    key_findings: list[str] = Field(
        default_factory=list,
        description="Main findings from the investigation",
        max_length=10
    )
    uncertainty_areas: list[str] = Field(
        default_factory=list,
        description="Areas where the investigation is uncertain",
        max_length=5
    )
    final_similarity: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Final similarity score achieved"
    )


class InvestigationStep(BaseModel):
    """A single step in the investigation process."""

    iteration: int = Field(..., ge=1, description="Iteration number")
    questions_asked: list[str] = Field(
        ...,
        description="Questions asked in this iteration"
    )
    best_similarity: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Best similarity score in this iteration"
    )
    phase: Literal["exploration", "refinement", "convergence"] = Field(
        ...,
        description="Current investigation phase"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="When this step occurred"
    )
    improvement: Optional[float] = Field(
        default=None,
        description="Similarity improvement from previous iteration"
    )


class InvestigationContext(BaseModel):
    """Context information for the current investigation."""

    current_description: str = Field(
        ...,
        description="Current best description of the embedding"
    )
    current_similarity: float = Field(
        ...,
        ge=0.0,
        le=1.0,
        description="Current similarity score"
    )
    phase: Literal["exploration", "refinement", "convergence"] = Field(
        ...,
        description="Current investigation phase"
    )
    previous_questions: list[str] = Field(
        default_factory=list,
        description="Previously asked questions to avoid repetition",
        max_length=50
    )
    iteration: int = Field(..., ge=1, description="Current iteration number")

    def get_recent_questions(self, n: int = 10) -> list[str]:
        """Get the n most recent questions."""
        return self.previous_questions[-n:]


class LLMProviderInfo(BaseModel):
    """Information about an LLM provider."""

    provider_name: str = Field(..., description="Name of the provider")
    model_name: str = Field(..., description="Name of the model")
    supports_streaming: bool = Field(default=False)
    supports_function_calling: bool = Field(default=False)
    max_tokens: int = Field(..., gt=0)
    temperature: float = Field(..., ge=0.0, le=2.0)
    is_available: bool = Field(..., description="Whether the provider is currently available")


class HealthCheckResult(BaseModel):
    """Result of a provider health check."""

    status: Literal["healthy", "unhealthy", "degraded"] = Field(
        ...,
        description="Health status"
    )
    provider_info: Optional[LLMProviderInfo] = None
    error_message: Optional[str] = None
    response_time_ms: Optional[float] = Field(
        default=None,
        ge=0.0,
        description="Response time in milliseconds"
    )
    timestamp: datetime = Field(
        default_factory=datetime.now,
        description="When the health check was performed"
    )
